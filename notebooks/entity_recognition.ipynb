{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import re\n",
    "import math\n",
    "from typing import List, Optional\n",
    "\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "# Transformer libraries\n",
    "from sentence_transformers import SentenceTransformer # For estimating the distance between (sub)sequences\n",
    "from sentence_transformers import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-trf==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.2.0/en_core_web_trf-3.2.0-py3-none-any.whl (460.2 MB)\n",
      "     |████████████████████████████████| 460.2 MB 87 kB/s              \n",
      "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /home/aumaron/Desktop/nlp/nlp_playground/nlp_env/lib/python3.9/site-packages (from en-core-web-trf==3.2.0) (3.2.0)\n",
      "Requirement already satisfied: spacy-transformers<1.2.0,>=1.1.2 in /home/aumaron/Desktop/nlp/nlp_playground/nlp_env/lib/python3.9/site-packages (from en-core-web-trf==3.2.0) (1.1.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/aumaron/Desktop/nlp/nlp_playground/nlp_env/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (2.26.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/aumaron/Desktop/nlp/nlp_playground/nlp_env/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (4.62.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/aumaron/Desktop/nlp/nlp_playground/nlp_env/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (21.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/aumaron/Desktop/nlp/nlp_playground/nlp_env/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/aumaron/Desktop/nlp/nlp_playground/nlp_env/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (1.0.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /home/aumaron/Desktop/nlp/nlp_playground/nlp_env/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (3.0.8)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /home/aumaron/Desktop/nlp/nlp_playground/nlp_env/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (2.4.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/aumaron/Desktop/nlp/nlp_playground/nlp_env/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (0.7.5)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/aumaron/Desktop/nlp/nlp_playground/nlp_env/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (0.6.1)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /home/aumaron/Desktop/nlp/nlp_playground/nlp_env/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (0.4.0)\n",
      "Requirement already satisfied: setuptools in /home/aumaron/Desktop/nlp/nlp_playground/nlp_env/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (59.5.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/aumaron/Desktop/nlp/nlp_playground/nlp_env/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/aumaron/Desktop/nlp/nlp_playground/nlp_env/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/aumaron/Desktop/nlp/nlp_playground/nlp_env/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (1.21.4)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /home/aumaron/Desktop/nlp/nlp_playground/nlp_env/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (0.8.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /home/aumaron/Desktop/nlp/nlp_playground/nlp_env/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: jinja2 in /home/aumaron/Desktop/nlp/nlp_playground/nlp_env/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (3.0.3)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/aumaron/Desktop/nlp/nlp_playground/nlp_env/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (1.0.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/aumaron/Desktop/nlp/nlp_playground/nlp_env/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /home/aumaron/Desktop/nlp/nlp_playground/nlp_env/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (8.0.13)\n",
      "Requirement already satisfied: transformers<4.12.0,>=3.4.0 in /home/aumaron/Desktop/nlp/nlp_playground/nlp_env/lib/python3.9/site-packages (from spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.2.0) (4.11.3)\n",
      "Requirement already satisfied: spacy-alignments<1.0.0,>=0.7.2 in /home/aumaron/Desktop/nlp/nlp_playground/nlp_env/lib/python3.9/site-packages (from spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.2.0) (0.8.4)\n",
      "Requirement already satisfied: torch>=1.6.0 in /home/aumaron/Desktop/nlp/nlp_playground/nlp_env/lib/python3.9/site-packages (from spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.2.0) (1.10.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/aumaron/Desktop/nlp/nlp_playground/nlp_env/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /home/aumaron/Desktop/nlp/nlp_playground/nlp_env/lib/python3.9/site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/aumaron/Desktop/nlp/nlp_playground/nlp_env/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (4.0.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/aumaron/Desktop/nlp/nlp_playground/nlp_env/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/aumaron/Desktop/nlp/nlp_playground/nlp_env/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/aumaron/Desktop/nlp/nlp_playground/nlp_env/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/aumaron/Desktop/nlp/nlp_playground/nlp_env/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (2.0.8)\n",
      "Requirement already satisfied: huggingface-hub>=0.0.17 in /home/aumaron/Desktop/nlp/nlp_playground/nlp_env/lib/python3.9/site-packages (from transformers<4.12.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.2.0) (0.1.2)\n",
      "Requirement already satisfied: filelock in /home/aumaron/Desktop/nlp/nlp_playground/nlp_env/lib/python3.9/site-packages (from transformers<4.12.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.2.0) (3.4.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/aumaron/Desktop/nlp/nlp_playground/nlp_env/lib/python3.9/site-packages (from transformers<4.12.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.2.0) (2021.11.10)\n",
      "Requirement already satisfied: sacremoses in /home/aumaron/Desktop/nlp/nlp_playground/nlp_env/lib/python3.9/site-packages (from transformers<4.12.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.2.0) (0.0.46)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/aumaron/Desktop/nlp/nlp_playground/nlp_env/lib/python3.9/site-packages (from transformers<4.12.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.2.0) (6.0)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/aumaron/Desktop/nlp/nlp_playground/nlp_env/lib/python3.9/site-packages (from transformers<4.12.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.2.0) (0.10.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/aumaron/Desktop/nlp/nlp_playground/nlp_env/lib/python3.9/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/aumaron/Desktop/nlp/nlp_playground/nlp_env/lib/python3.9/site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (2.0.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: joblib in /home/aumaron/Desktop/nlp/nlp_playground/nlp_env/lib/python3.9/site-packages (from sacremoses->transformers<4.12.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.2.0) (1.1.0)\n",
      "Requirement already satisfied: six in /home/aumaron/Desktop/nlp/nlp_playground/nlp_env/lib/python3.9/site-packages (from sacremoses->transformers<4.12.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.2.0) (1.16.0)\n",
      "Installing collected packages: en-core-web-trf\n",
      "Successfully installed en-core-web-trf-3.2.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_trf')\n"
     ]
    }
   ],
   "source": [
    "# Instantiate SBERT\n",
    "sentence_model = SentenceTransformer('all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27696/1039543407.py:8: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  glove2word2vec(glove_file, word2vec_glove_file)\n"
     ]
    }
   ],
   "source": [
    "# Load spacy language model\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "# Load the GloVe embeddings\n",
    "glove_file = datapath('/home/aum/Desktop/projects/nlp/models/glove.6B.100d.txt')\n",
    "# glove_file = datapath('/home/aumaron/Desktop/nlp/nlp_playground/models/glove.6B/glove.6B.100d.txt')\n",
    "word2vec_glove_file = get_tmpfile(\"glove.6B.100d.word2vec.txt\")\n",
    "glove2word2vec(glove_file, word2vec_glove_file)\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(word2vec_glove_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dimensional space of the embeddings\n",
    "\n",
    "model[model.index_to_key[0]].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_data = pd.read_excel(\"../data/entity_data.xlsx\")\n",
    "root_word_corpus = pd.read_excel(\"../data/root_word_corpus.xlsx\")\n",
    "column_names = entity_data[\"column_names\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_corpus = root_word_corpus[['id', 'name', 'entity']].to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order id\n"
     ]
    }
   ],
   "source": [
    "column_name = 'order_id'\n",
    "column_name =  re.sub(r'[@_!#$%^&*()<>?[\\]./\\\\|}{~:-]', ' ', column_name)  # Removal of special characters\n",
    "column_name = re.sub(r\"[ ]{2,}\", \" \", column_name)  # Remove additional spaces\n",
    "print(column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "\n",
    "query_embedding = sentence_model.encode([column_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Find NER using Spacy\n",
    "2. Find individual tokens in intermediate corpus\n",
    "3. If not found in step 2, find semantically similar words in R^d 100 dimensional space\n",
    "4. Future scope:\n",
    "    % is removed as a character\n",
    "    Certain columns containing '%' in the beginning or end are percentage columns\n",
    "    Need to add the exception for %\n",
    "    \n",
    "5. Challenges -\n",
    "    - False positives in model based NER\n",
    "    \n",
    "6. Future scope and experimentation\n",
    "    - Try 200d vectors\n",
    "    - Try 300d vectors\n",
    "    - Try 768d BERT embeddings (non context specific word embeddings)\n",
    "    - Try extracting phrase (in this case column names) embeddings as a whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "\n",
    "def model_based_ner(string: str) -> list:\n",
    "    word = nlp(column_name)\n",
    "    entity_list = []\n",
    "    for token in word:\n",
    "        print(f\"{token.ent_iob_} -> {token.ent_type_}\")\n",
    "        if token.ent_type_:\n",
    "            entity_list.append(token.ent_type_)\n",
    "    \n",
    "    return entity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "\n",
    "def find_in_corpus(root_word: str, word_corpus: List[dict]) -> list:\n",
    "    entity_list = []\n",
    "    filtered_list = list(filter(lambda word_meta: word_meta[\"name\"] == root_word, word_corpus))\n",
    "    if filtered_list:\n",
    "        for each_object in filtered_list:\n",
    "            entity_list.append({each_object.get(\"name\"): each_object.get(\"entity\")})\n",
    "    else:\n",
    "        entity_list = [{root_word: \"\"}]\n",
    "    \n",
    "    return entity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "\n",
    "def construct_corpus_matrix(word_corpus: List[dict], embedding_model):\n",
    "    word_array = np.empty([len(word_corpus), embedding_model[embedding_model.index_to_key[0]].shape[0]])\n",
    "    for row_number, root in enumerate(word_corpus):\n",
    "        try:\n",
    "            word_array[row_number, :] = embedding_model.get_vector(root.get(\"name\"))\n",
    "        except KeyError:\n",
    "            word_array[row_number, :] = np.zeros([100,])\n",
    "        \n",
    "    return word_array\n",
    "\n",
    "# corpus_array = construct_corpus_matrix(intermediate_corpus, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "\n",
    "def embedding_product(a, b):\n",
    "    cos_theta = a.dot(b.T)/(np.sqrt(np.sum(np.square(a)))*(np.sqrt(np.sum(np.square(b)))))\n",
    "    _angle = math.acos(cos_theta)\n",
    "\n",
    "    return cos_theta, _angle\n",
    "\n",
    "\n",
    "def get_closest_word(root_word_embedding: np.ndarray, \n",
    "                     matrix: np.ndarray,\n",
    "                     confidence_required: Optional[float] = 0.5):\n",
    "    theta_list = []\n",
    "    angle_list = []\n",
    "    for column_vec in range(matrix.T.shape[1]):\n",
    "        doc_product, angle_between_vectors = embedding_product(root_word_embedding, matrix.T[:, column_vec])\n",
    "        theta_list.append(doc_product)\n",
    "        angle_list.append(angle_between_vectors)  # If needed for validation\n",
    "    \n",
    "    # Cut-off: filter theta list based on the confidence required\n",
    "    filtered_theta_index_list = [theta_list.index(score) for score in theta_list if np.abs(score)>=confidence_required]\n",
    "    \n",
    "    # Find index of the top score\n",
    "    closest_vector = None\n",
    "    if filtered_theta_index_list:\n",
    "        closest_vector = theta_list.index(max([theta_list[filter_index] for filter_index in filtered_theta_index_list]))\n",
    "        \n",
    "#     print(theta_list)\n",
    "#     print(theta_list.index(max(theta_list)))\n",
    "#     print(theta_list[theta_list.index(max(theta_list))])\n",
    "#     print(angle_list.index(min(angle_list)))\n",
    "    \n",
    "    return closest_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O -> \n",
      "O -> \n",
      "O -> \n",
      "Model NER ->  []\n",
      "Simple search in Corpus ->  [{'order': ''}, {'id': ''}]\n",
      "Last word entity ->  [{'id': ''}]\n",
      "Updated entity object ->  [{'order': ''}, {'id': {'id': 35, 'name': 'user', 'entity': 'Person'}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27696/1478938175.py:4: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  cos_theta = a.dot(b.T)/(np.sqrt(np.sum(np.square(a)))*(np.sqrt(np.sum(np.square(b)))))\n"
     ]
    }
   ],
   "source": [
    "# Step 0\n",
    "corpus_array = construct_corpus_matrix(intermediate_corpus, model)\n",
    "# print(corpus_array.shape)\n",
    "\n",
    "# Step 1 \n",
    "entity_from_model = model_based_ner(column_name)\n",
    "print('Model NER -> ', entity_from_model)\n",
    "\n",
    "# Step 2: Find in corpus\n",
    "if not entity_from_model:\n",
    "    word_split = column_name.split(\" \")\n",
    "    # Step 2.a: Find sub_words in corpus\n",
    "    entity_from_corpus = []\n",
    "    for word in word_split:\n",
    "        entity_from_corpus.extend(find_in_corpus(word, intermediate_corpus))  # Can be replaced using a mat mul\n",
    "    print('Simple search in Corpus -> ', entity_from_corpus)\n",
    "    \n",
    "    # Step 2.b: Find if the last sub_word has returned an entity\n",
    "    # Note: There can be 4 possibilities:\n",
    "        # 1. All sub words can return entity\n",
    "        # 2. Any sub_word other than the trailing sub_word returns an entity\n",
    "        # 3. Any sub_word including the trailing sub_word returns an entity\n",
    "        # 4. None of them return an entity\n",
    "    \n",
    "    # 2.b.1: Check if all words contain\n",
    "    # 2.b.2: Check if last word has empty entity\n",
    "    last_sub_word_entity = list(filter(lambda word_is: word_is.get(word_split[-1]) == \"\", entity_from_corpus))\n",
    "    print('Last word entity -> ', last_sub_word_entity)\n",
    "    \n",
    "    # If non-empty, then check this word in corpus\n",
    "    if last_sub_word_entity:\n",
    "        theta_list = []\n",
    "        last_word_embedding = model.get_vector(word_split[-1])\n",
    "        closest_index = get_closest_word(last_word_embedding.squeeze()[:100], corpus_array, 0)\n",
    "        closest_entity = intermediate_corpus[closest_index] if (closest_index or closest_index == 0) else \"\"\n",
    "        entity_from_corpus[-1].update({word_split[-1]: closest_entity[]})\n",
    "        print('Updated entity object -> ', entity_from_corpus)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# corpus_array = construct_corpus_matrix(intermediate_corpus, model)\n",
    "# corpus_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 26, 'name': 'vendor', 'entity': 'Organisation'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intermediate_corpus[26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.83234936 0.5874632370639816\n"
     ]
    }
   ],
   "source": [
    "sneaker = model.get_vector(\"man\")\n",
    "shoes = model.get_vector(\"woman\")\n",
    "\n",
    "dot_prod, angle = vector_cosine(sneaker, shoes)\n",
    "print(dot_prod, angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3502c2d8a44f1d370a9f7e349ee408832b67c21c31bd8924b7b4cde15de44893"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('nlp_env': venv)",
   "language": "python",
   "name": "python391jvsc74a57bd03d2a12883df202f51a08e1f757b345e71b08bf7c8dc91b1538c0cba0bebac7b3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
