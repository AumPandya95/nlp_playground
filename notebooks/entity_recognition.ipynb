{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import re\n",
    "import math\n",
    "import json\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional\n",
    "\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "# Transformer libraries\n",
    "from sentence_transformers import SentenceTransformer # For estimating the distance between (sub)sequences\n",
    "from sentence_transformers import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate SBERT\n",
    "sentence_model = SentenceTransformer('all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30379/1039543407.py:8: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  glove2word2vec(glove_file, word2vec_glove_file)\n"
     ]
    }
   ],
   "source": [
    "# Load spacy language model\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "# Load the GloVe embeddings\n",
    "glove_file = datapath('/home/aum/Desktop/projects/nlp/models/glove.6B.100d.txt')\n",
    "# glove_file = datapath('/home/aumaron/Desktop/nlp/nlp_playground/models/glove.6B/glove.6B.100d.txt')\n",
    "word2vec_glove_file = get_tmpfile(\"glove.6B.100d.word2vec.txt\")\n",
    "glove2word2vec(glove_file, word2vec_glove_file)\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(word2vec_glove_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dimensional space of the embeddings\n",
    "\n",
    "model[model.index_to_key[0]].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_data = pd.read_excel(\"../data/entity_data.xlsx\")\n",
    "root_word_corpus = pd.read_excel(\"../data/root_word_corpus.xlsx\")\n",
    "column_names = entity_data[\"column_names\"].tolist()\n",
    "old_entity_corpus = open(\"../misc/corpus_transformed.json\")\n",
    "old_entity_corpus = json.load(old_entity_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_corpus = root_word_corpus[['id', 'name', 'entity']].to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "branch name\n"
     ]
    }
   ],
   "source": [
    "column_name = 'branch/name'\n",
    "column_name =  re.sub(r'[@_!#$%^&*()<>?[\\]./\\\\+|}{~:-]', ' ', column_name)  # Removal of special characters\n",
    "column_name = re.sub(r\"[ ]{2,}\", \" \", column_name)  # Remove additional spaces\n",
    "print(column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "query_embedding = sentence_model.encode([column_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Find NER using Spacy\n",
    "2. Find individual tokens in intermediate corpus\n",
    "3. If not found in step 2, find semantically similar words in R^d 100 dimensional space\n",
    "4. Future scope:\n",
    "    % is removed as a character\n",
    "    Certain columns containing '%' in the beginning or end are percentage columns\n",
    "    Need to add the exception for %\n",
    "    \n",
    "5. Challenges -\n",
    "    - False positives in model based NER\n",
    "    - False positives in new algorithmic approach\n",
    "    - Reducing the prediction window (lesser, but more accurate entities should be displayed)\n",
    "    \n",
    "6. Future scope and experimentation\n",
    "    - Implement a heuristic algorithm to funnel out wrong predictions from the predicted universe\n",
    "        - Syntactic rules can be generalised and implemented\n",
    "            1. Eg. \"Date of user joining\" \n",
    "                \"of\" is an adposition (ADP) which we can use to decipher that the prior entity is an important keyword in the entity; which is \"Date\"\n",
    "            2. Eg. \"units in inventory\";\n",
    "                \"Units\" -> relates to \"Sales\";\n",
    "                \"in\" -> is the Adposition;\n",
    "                \"inventory\" -> relates to \"Sales\" and \"Product\";\n",
    "                But we eliminate Product as we weigh \"Units\" more in terms of entity\n",
    "                \n",
    "    - Use randomly selected values to funnel out entities further\n",
    "    - Try 200d vectors\n",
    "    - Try 300d vectors\n",
    "    - Try 768d BERT embeddings (non context specific word embeddings)\n",
    "    - Try extracting phrase (in this case column names) embeddings as a whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "\n",
    "def model_based_ner(string: str) -> list:\n",
    "    word = nlp(column_name)\n",
    "    entity_list = []\n",
    "    for token_num, token in enumerate(word):\n",
    "#         print(f\"{token.text} -> {token.ent_iob_} -> {token.ent_type_}\")\n",
    "        print(f\"{token.text} -> {token.pos_}\")\n",
    "        if token.ent_type_:\n",
    "            entity_list.append(token.ent_type_)\n",
    "    \n",
    "    return entity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "\n",
    "@dataclass\n",
    "class CorpusSearch:\n",
    "    column_name: str\n",
    "    word_corpus: List[dict]\n",
    "    word_matrix: np.ndarray\n",
    "    confidence_required: float\n",
    "    embedding_model: KeyedVectors\n",
    "    entity_from_corpus: list = field(default_factory=list)\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        word_split = self.column_name.split(\" \")\n",
    "        for word_num, word in enumerate(word_split):\n",
    "            self.word_id = word_num\n",
    "            self.root_word = word\n",
    "            try:\n",
    "                self.root_word_embedding = self.embedding_model.get_vector(self.root_word)\n",
    "            except KeyError:\n",
    "                self.root_word_embedding = np.zeros((100,))\n",
    "            self.last_word_embedding = self.root_word_embedding\n",
    "            self.entity_from_corpus.extend(self.find_in_corpus())\n",
    "    \n",
    "    def find_in_corpus(self) -> list:\n",
    "        entity_list = []\n",
    "        filtered_list = list(filter(lambda word_meta: word_meta[\"name\"] == self.root_word, \n",
    "                                    self.word_corpus))\n",
    "        meta_obj = {\n",
    "            \"actual_word\": self.root_word,\n",
    "            \"entity\": [],\n",
    "            \"closest_root:\": self.root_word,\n",
    "            \"word_id\": self.word_id,\n",
    "            \"confidence\": []\n",
    "        }\n",
    "        if filtered_list:\n",
    "            for each_object in filtered_list:\n",
    "                meta_obj[\"entity\"].append(each_object.get(\"entity\"))\n",
    "                meta_obj[\"confidence\"].append(1.)\n",
    "            entity_list.append(meta_obj)\n",
    "        else:\n",
    "            closest_index, confidence_value = self.get_closest_word()\n",
    "            if closest_index or closest_index == 0:\n",
    "                closest_entity = self.word_corpus[closest_index] if (closest_index or closest_index == 0) else \"\"\n",
    "                entity_list = [{\"actual_word\": self.root_word,\n",
    "                                \"entity\": [closest_entity[\"entity\"]],\n",
    "                                \"closest_root\": closest_entity[\"name\"],\n",
    "                                \"word_id\": self.word_id,\n",
    "                                \"confidence\": [confidence_value]}]\n",
    "            else:\n",
    "                entity_list = [{\"actual_word\": self.root_word,\n",
    "                                \"entity\": [],\n",
    "                                \"closest_root\": \"\",\n",
    "                                \"word_id\": self.word_id,\n",
    "                                \"confidence\": [confidence_value]}]\n",
    "\n",
    "        return entity_list\n",
    "    \n",
    "    @staticmethod\n",
    "    def embedding_product(a, b):\n",
    "        cos_theta = a.dot(b.T)/(np.sqrt(np.sum(np.square(a)))*(np.sqrt(np.sum(np.square(b)))))\n",
    "        _angle = math.acos(cos_theta)\n",
    "\n",
    "        return cos_theta, _angle\n",
    "\n",
    "\n",
    "    def get_closest_word(self):\n",
    "        theta_list = []\n",
    "        angle_list = []\n",
    "        for column_vec in range(self.word_matrix.T.shape[1]):\n",
    "            doc_product, angle_between_vectors = self.embedding_product(self.last_word_embedding, \n",
    "                                                                        self.word_matrix.T[:, column_vec])\n",
    "            theta_list.append(doc_product)\n",
    "            angle_list.append(angle_between_vectors)  # If needed for validation\n",
    "\n",
    "        # Cut-off: filter theta list based on the confidence required\n",
    "        filtered_theta_index_list = [\n",
    "            theta_list.index(score) for score in theta_list if np.abs(score)>=self.confidence_required]\n",
    "\n",
    "        # Find index of the top score\n",
    "        closest_vector = None\n",
    "        theta_value = None\n",
    "        if filtered_theta_index_list:\n",
    "            closest_vector = theta_list.index(max([theta_list[filter_index] for filter_index in filtered_theta_index_list]))\n",
    "            theta_value = theta_list[closest_vector]\n",
    "#         print(\"closest_vect\",closest_vector)\n",
    "    #     print(theta_list)\n",
    "#         print(theta_list.index(max(theta_list)))\n",
    "    #     print(theta_list[theta_list.index(max(theta_list))])\n",
    "    #     print(angle_list.index(min(angle_list)))\n",
    "\n",
    "        return closest_vector, theta_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "\n",
    "def construct_corpus_matrix(word_corpus: List[dict], embedding_model):\n",
    "    word_array = np.empty([len(word_corpus), embedding_model[embedding_model.index_to_key[0]].shape[0]])\n",
    "    for row_number, root in enumerate(word_corpus):\n",
    "        try:\n",
    "            word_array[row_number, :] = embedding_model.get_vector(root.get(\"name\"))\n",
    "        except KeyError:\n",
    "            word_array[row_number, :] = np.zeros([100,])\n",
    "        \n",
    "    return word_array\n",
    "\n",
    "# corpus_array = construct_corpus_matrix(intermediate_corpus, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "units -> NOUN\n",
      "in -> ADP\n",
      "inventory -> NOUN\n",
      "Model NER ->  []\n",
      "Search in Corpus ->  [{'actual_word': 'units', 'entity': ['Sales'], 'closest_root': 'supply', 'word_id': 0, 'confidence': [0.5354936262980426]}, {'actual_word': 'in', 'entity': ['Location'], 'closest_root': 'place', 'word_id': 1, 'confidence': [0.7434392096144258]}, {'actual_word': 'inventory', 'entity': ['Sales', 'Product'], 'closest_root:': 'inventory', 'word_id': 2, 'confidence': [1.0, 1.0]}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30379/1396784973.py:84: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  cos_theta = a.dot(b.T)/(np.sqrt(np.sum(np.square(a)))*(np.sqrt(np.sum(np.square(b)))))\n"
     ]
    }
   ],
   "source": [
    "# Step 0\n",
    "corpus_array = construct_corpus_matrix(intermediate_corpus, model)\n",
    "# print(corpus_array.shape)\n",
    "\n",
    "entity_from_old_method = []\n",
    "entity_from_model_list = []\n",
    "entity_from_search = []\n",
    "for column_name in [\"units in inventory\"]:  # entity_data[\"column_names\"].values:\n",
    "    old_entity_list = [\n",
    "        word_obj[\"entity\"] for word_obj in list(filter(lambda corpus: corpus.get(\"name\") == column_name, old_entity_corpus))\n",
    "    ]\n",
    "#     print(\"old entities\", old_entity_list)\n",
    "    old_entity_list = list(set(old_entity_list))\n",
    "    if not old_entity_list:\n",
    "        old_entity_list = [\"\"]\n",
    "    entity_from_old_method.append(old_entity_list)\n",
    "    column_name = column_name.lower()\n",
    "    column_name =  re.sub(r'[@_!#$%^&*()<>?[\\]./\\\\+|}{~:-]', ' ', column_name)  # Removal of special characters\n",
    "    column_name = re.sub(r\"[ ]{2,}\", \" \", column_name)  # Remove additional spaces\n",
    "    # Step 1 \n",
    "    entity_from_model = model_based_ner(column_name)\n",
    "    print('Model NER -> ', entity_from_model)\n",
    "    if entity_from_model:\n",
    "        entity_from_model = list(set(entity_from_model))\n",
    "    else:\n",
    "        entity_from_model = [\"\"]\n",
    "    entity_from_model_list.append(entity_from_model)\n",
    "\n",
    "    # Step 2: Find in corpus\n",
    "    corpus_search_obj = CorpusSearch(\n",
    "        column_name=column_name,\n",
    "        word_corpus=intermediate_corpus,\n",
    "        word_matrix=corpus_array,\n",
    "        confidence_required=0.5,\n",
    "        embedding_model=model,\n",
    "    )\n",
    "\n",
    "    print('Search in Corpus -> ', corpus_search_obj.entity_from_corpus)\n",
    "\n",
    "    # Check if no entities found\n",
    "    non_identified_entity = list(filter(lambda word_is: not word_is.get(\"entity\"), \n",
    "                                        corpus_search_obj.entity_from_corpus))\n",
    "    if non_identified_entity and (len(non_identified_entity) == len(column_name.split(\" \"))):\n",
    "        print(f\"--------\\nNo entities found!\\n--------\")\n",
    "\n",
    "    extract_entities = [word_obj[\"entity\"] for word_obj in corpus_search_obj.entity_from_corpus]\n",
    "    extract_entities = [word for word_list in extract_entities for word in list(set(word_list))]\n",
    "#     print(extract_entities)\n",
    "    \n",
    "    entity_from_search.append(\", \".join(extract_entities))\n",
    "    for i, word in enumerate(entity_from_search):\n",
    "        if word == \"\":\n",
    "            entity_from_search.pop(i)\n",
    "            entity_from_search.insert(i, \"Default\")\n",
    "\n",
    "\n",
    "# entity_from_old_method = [word for word_list in entity_from_old_method for word in word_list]\n",
    "# entity_from_model_list = [word for word_list in entity_from_model_list for word in word_list]\n",
    "\n",
    "# entity_data[\"old_recognition_method\"] = entity_from_old_method\n",
    "# entity_data[\"model_based_recognition\"] = entity_from_model_list\n",
    "# entity_data[\"new_algorithm_based_recognition\"] = entity_from_search\n",
    "\n",
    "# if not entity_from_model:\n",
    "#     word_split = column_name.split(\" \")\n",
    "#     # Step 2.a: Find sub_words in corpus\n",
    "#     entity_from_corpus = []\n",
    "#     for word_num, word in enumerate(word_split):\n",
    "#         entity_from_corpus.extend(find_in_corpus(word, word_num, intermediate_corpus))  # Can be replaced using a mat mul\n",
    "#     print('Simple search in Corpus -> ', entity_from_corpus)\n",
    "    \n",
    "#     # Step 2.b: Find if the last sub_word has returned an entity\n",
    "#     # Note: There can be 4 possibilities:\n",
    "#         # 1. All sub words can return entity\n",
    "#         # 2. Any sub_word other than the trailing sub_word returns an entity\n",
    "#         # 3. Any sub_word including the trailing sub_word returns an entity\n",
    "#         # 4. None of them return an entity\n",
    "    \n",
    "#     # 2.b.1: Check if all words contain entity\n",
    "#     non_identified_entity = list(filter(lambda word_is: not word_is.get(\"entity\"), entity_from_corpus))\n",
    "#     if non_identified_entity and (len(non_identified_entity) == len(word_split)):\n",
    "#         print(f\"--------\\nNo entities found!\\n--------\")\n",
    "        \n",
    "#     # 2.b.2:\n",
    "    \n",
    "    \n",
    "#     # 2.b.2: Check if last word has empty entity\n",
    "#     last_sub_word_entity = list(filter(\n",
    "#         lambda word_is: (word_is.get(\"actual_word\") == word_split[-1]) and (not word_is.get(\"entity\")), \n",
    "#         entity_from_corpus))\n",
    "#     print('Last word entity -> ', last_sub_word_entity)\n",
    "    \n",
    "#     # If non-empty, then check this word in corpus\n",
    "#     if last_sub_word_entity:\n",
    "#         theta_list = []\n",
    "#         word_id = last_sub_word_entity[-1].get(\"word_id\")\n",
    "#         last_word_embedding = model.get_vector(word_split[-1])\n",
    "#         closest_index = get_closest_word(last_word_embedding.squeeze()[:100], corpus_array, 0.3)\n",
    "        \n",
    "#         if closest_index:\n",
    "#             closest_entity = intermediate_corpus[closest_index] if (closest_index or closest_index == 0) else \"\"\n",
    "#             entity_from_corpus[-1].update({\"actual_word\": word_split[-1],\n",
    "#                                            \"entity\": [closest_entity[\"entity\"]],\n",
    "#                                            \"closest_root\": closest_entity[\"name\"],\n",
    "#                                            \"word_id\": word_id})\n",
    "#         else:\n",
    "#             print(f\"--------\\nNo entities found!\\n--------\")\n",
    "        \n",
    "#         print('Updated entity object -> ', entity_from_corpus)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# # corpus_array = construct_corpus_matrix(intermediate_corpus, model)\n",
    "# # corpus_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nation', 0.8758761882781982),\n",
       " ('now', 0.7629706263542175),\n",
       " ('well', 0.746636688709259),\n",
       " ('countries', 0.7459027171134949),\n",
       " ('world', 0.7444546818733215),\n",
       " ('states', 0.7400078773498535),\n",
       " ('has', 0.7369707822799683),\n",
       " ('government', 0.73195481300354),\n",
       " ('already', 0.7283994555473328),\n",
       " ('most', 0.7282326221466064),\n",
       " ('part', 0.7277618646621704),\n",
       " ('still', 0.7275569438934326),\n",
       " ('today', 0.7264338731765747),\n",
       " ('united', 0.726360023021698),\n",
       " ('all', 0.7252687811851501),\n",
       " ('over', 0.7178832292556763),\n",
       " ('this', 0.7172205448150635),\n",
       " ('europe', 0.7167598009109497),\n",
       " ('the', 0.7163448333740234),\n",
       " ('people', 0.7160573601722717),\n",
       " ('and', 0.7149220108985901),\n",
       " ('america', 0.7145408987998962),\n",
       " ('since', 0.7144755721092224),\n",
       " ('it', 0.7130166888237),\n",
       " ('region', 0.7127728462219238),\n",
       " ('once', 0.7126500606536865),\n",
       " ('one', 0.7116884589195251),\n",
       " ('in', 0.7093125581741333),\n",
       " ('here', 0.7091266512870789),\n",
       " ('but', 0.7043055891990662),\n",
       " ('years', 0.7039714455604553),\n",
       " ('even', 0.7038114070892334),\n",
       " ('only', 0.7038098573684692),\n",
       " ('far', 0.7030112147331238),\n",
       " ('nations', 0.7016267776489258),\n",
       " ('among', 0.7013375759124756),\n",
       " ('as', 0.7008084654808044),\n",
       " ('because', 0.7000370025634766),\n",
       " ('is', 0.6985384225845337),\n",
       " ('been', 0.6981486678123474),\n",
       " ('especially', 0.697928249835968),\n",
       " ('not', 0.6977711915969849),\n",
       " ('more', 0.6971189379692078),\n",
       " ('would', 0.6951038837432861),\n",
       " ('many', 0.6949200630187988),\n",
       " ('way', 0.694879412651062),\n",
       " ('decades', 0.6944616436958313),\n",
       " ('rest', 0.6930471658706665),\n",
       " ('so', 0.6919248700141907),\n",
       " ('despite', 0.6915313005447388)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similar_by_word(\"country\", topn=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_data.to_excel(\"validation_output_final_.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'actual_word': 'kk', 'entity': [], 'closest_root': '', 'word_id': 0},\n",
       " {'actual_word': 'kk', 'entity': [], 'closest_root': '', 'word_id': 1}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_search_obj.entity_from_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 26, 'name': 'company', 'entity': 'Organisation'}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intermediate_corpus[26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8785]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "util.cos_sim(model.get_vector(\"organization\"), model.get_vector(\"organisation\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.83234936 0.5874632370639816\n"
     ]
    }
   ],
   "source": [
    "sneaker = model.get_vector(\"man\")\n",
    "shoes = model.get_vector(\"woman\")\n",
    "\n",
    "dot_prod, angle = vector_cosine(sneaker, shoes)\n",
    "print(dot_prod, angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3502c2d8a44f1d370a9f7e349ee408832b67c21c31bd8924b7b4cde15de44893"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('nlp_env': venv)",
   "language": "python",
   "name": "python391jvsc74a57bd03502c2d8a44f1d370a9f7e349ee408832b67c21c31bd8924b7b4cde15de44893"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
