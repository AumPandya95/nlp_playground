{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_file = datapath('/home/aum/Desktop/projects/nlp/models/glove.6B.100d.txt')\n",
    "# glove_file = datapath('/home/aumaron/Desktop/nlp/nlp_playground/models/glove.6B/glove.6B.100d.txt')\n",
    "word2vec_glove_file = get_tmpfile(\"glove.6B.100d.word2vec.txt\")\n",
    "glove2word2vec(glove_file, word2vec_glove_file)\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(word2vec_glove_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[model.index_to_key[0]].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_data = pd.read_excel(\"../data/entity_data.xlsx\")\n",
    "root_word_corpus = pd.read_excel(\"../data/root_word_corpus.xlsx\")\n",
    "column_names = entity_data[\"column_names\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_corpus = root_word_corpus[['id', 'name', 'entity']].to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "branch manager\n"
     ]
    }
   ],
   "source": [
    "column_name = 'branch_manager'\n",
    "column_name =  re.sub(r'[@_!#$%^&*()<>?[\\]./\\\\|}{~:-]', ' ', column_name)  # Removal of special characters\n",
    "column_name = re.sub(r\"[ ]{2,}\", \" \", column_name)  # Remove additional spaces\n",
    "print(column_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Find NER using Spacy\n",
    "2. Find individual tokens in intermediate corpus\n",
    "3. If not found in step 2, find semantically similar words in R^d 100 dimensional space\n",
    "4. Future scope:\n",
    "    % is removed as a character\n",
    "    Certain columns containing '%' in the beginning or end are percentage columns\n",
    "    Need to add the exception for %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "\n",
    "def model_based_ner(string: str) -> list:\n",
    "    word = nlp(column_name)\n",
    "    entity_list = []\n",
    "    for token in word:\n",
    "#         print(f\"{token.ent_iob_} -> {token.ent_type_}\")\n",
    "        if token.ent_type_:\n",
    "            entity_list.append(token.ent_type_)\n",
    "    \n",
    "    return entity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "\n",
    "def find_in_corpus(root_word: str, word_corpus: List[dict]) -> list:\n",
    "    entity_list = []\n",
    "    filtered_list = list(filter(lambda word_meta: word_meta[\"name\"] == root_word, word_corpus))\n",
    "    if filtered_list:\n",
    "        for each_object in filtered_list:\n",
    "            entity_list.append({each_object.get(\"name\"): each_object.get(\"entity\")})\n",
    "    else:\n",
    "        entity_list = []\n",
    "    \n",
    "    return entity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.2 µs ± 381 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# %%timeit\n",
    "\n",
    "def construct_corpus_matrix(word_corpus: List[dict], embedding_model):\n",
    "    word_array = np.empty([len(word_corpus), embedding_model[embedding_model.index_to_key[0]].shape[0]])\n",
    "    for row_number, root in enumerate(word_corpus):\n",
    "        try:\n",
    "            word_array[row_number, :] = embedding_model.get_vector(root.get(\"name\"))\n",
    "        except KeyError:\n",
    "            word_array[row_number, :] = np.zeros([100,])\n",
    "        \n",
    "    return word_array\n",
    "\n",
    "# corpus_array = construct_corpus_matrix(intermediate_corpus, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[{'branch': 'Location'}, {'branch': 'Organisation'}]\n",
      "(64, 100)\n"
     ]
    }
   ],
   "source": [
    "# Step 1 \n",
    "entity_list = model_based_ner(column_name)\n",
    "print(entity_list)\n",
    "\n",
    "# Step 2: Find \n",
    "if not entity_list:\n",
    "    word_split = column_name.split(\" \")\n",
    "    # Step 2.a: Find sub_words in corpus\n",
    "    entity_from_corpus = []\n",
    "    for word in word_split:\n",
    "        entity_from_corpus.extend(find_in_corpus(word, intermediate_corpus))\n",
    "    # Step 2.b: Find if the last sub_word has returned an entity\n",
    "    # Note: There can be 4 possibilities:\n",
    "        # 1. All sub words can return entity\n",
    "        # 2. Any sub_word other than the trailing sub_word returns an entity\n",
    "        # 3. Any sub_word including the trailing sub_word returns an entity\n",
    "        # 4. None of them return an entity\n",
    "        \n",
    "    print(entity_from_corpus)\n",
    "    corpus_array = construct_corpus_matrix(intermediate_corpus, model)\n",
    "    print(corpus_array.shape)\n",
    "\n",
    "\n",
    "# corpus_array = construct_corpus_matrix(intermediate_corpus, model)\n",
    "# corpus_array.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('nlp_env': venv)",
   "language": "python",
   "name": "python391jvsc74a57bd03502c2d8a44f1d370a9f7e349ee408832b67c21c31bd8924b7b4cde15de44893"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
