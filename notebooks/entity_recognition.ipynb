{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import re\n",
    "import math\n",
    "import json\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional\n",
    "\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "# Transformer libraries\n",
    "from sentence_transformers import SentenceTransformer # For estimating the distance between (sub)sequences\n",
    "from sentence_transformers import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate SBERT\n",
    "sentence_model = SentenceTransformer('all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m~/Desktop/nlp/nlp_playground/nlp_env/lib/python3.9/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36msave_word2vec_format\u001b[0;34m(self, fname, fvocab, binary, total_vec, write_header, prefix, append, sort_attr)\u001b[0m\n\u001b[1;32m   1579\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1580\u001b[0;31m                     \u001b[0mfout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{prefix}{key} {' '.join(repr(val) for val in key_vector)}\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13267/156734918.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mglove_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatapath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/aumaron/Desktop/nlp/nlp_playground/models/glove.6B/glove.6B.100d.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mword2vec_glove_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tmpfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"glove.6B.100d.word2vec.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mglove2word2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglove_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2vec_glove_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword2vec_glove_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/nlp/nlp_playground/nlp_env/lib/python3.9/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mnew_func1\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1519\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m                 )\n\u001b[0;32m-> 1521\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1523\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_func1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/nlp/nlp_playground/nlp_env/lib/python3.9/site-packages/gensim/scripts/glove2word2vec.py\u001b[0m in \u001b[0;36mglove2word2vec\u001b[0;34m(glove_input_file, word2vec_output_file)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mnum_lines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglovekv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglovekv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"converting %i vectors from %s to %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_lines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglove_input_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2vec_output_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0mglovekv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword2vec_output_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnum_lines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_dims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/nlp/nlp_playground/nlp_env/lib/python3.9/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36msave_word2vec_format\u001b[0;34m(self, fname, fvocab, binary, total_vec, write_header, prefix, append, sort_attr)\u001b[0m\n\u001b[1;32m   1578\u001b[0m                     \u001b[0mfout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{prefix}{key} \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mkey_vector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mREAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1579\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1580\u001b[0;31m                     \u001b[0mfout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{prefix}{key} {' '.join(repr(val) for val in key_vector)}\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1582\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device"
     ]
    }
   ],
   "source": [
    "# Load spacy language model\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "# Load the GloVe embeddings\n",
    "# glove_file = datapath('/home/aum/Desktop/projects/nlp/models/glove.6B.100d.txt')\n",
    "glove_file = datapath('/home/aumaron/Desktop/nlp/nlp_playground/models/glove.6B/glove.6B.100d.txt')\n",
    "word2vec_glove_file = get_tmpfile(\"glove.6B.100d.word2vec.txt\")\n",
    "glove2word2vec(glove_file, word2vec_glove_file)\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(word2vec_glove_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dimensional space of the embeddings\n",
    "\n",
    "model[model.index_to_key[0]].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_data = pd.read_excel(\"../data/entity_data.xlsx\")\n",
    "root_word_corpus = pd.read_excel(\"../data/root_word_corpus.xlsx\")\n",
    "column_names = entity_data[\"column_names\"].tolist()\n",
    "old_entity_corpus = open(\"../misc/corpus_transformed.json\")\n",
    "old_entity_corpus = json.load(old_entity_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_corpus = root_word_corpus[['id', 'name', 'entity']].to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "branch name\n"
     ]
    }
   ],
   "source": [
    "column_name = 'branch/name'\n",
    "column_name =  re.sub(r'[@_!#$%^&*()<>?[\\]./\\\\+|}{~:-]', ' ', column_name)  # Removal of special characters\n",
    "column_name = re.sub(r\"[ ]{2,}\", \" \", column_name)  # Remove additional spaces\n",
    "print(column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "query_embedding = sentence_model.encode([column_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Find NER using Spacy\n",
    "2. Find individual tokens in intermediate corpus\n",
    "3. If not found in step 2, find semantically similar words in R^d 100 dimensional space\n",
    "4. Future scope:\n",
    "    % is removed as a character\n",
    "    Certain columns containing '%' in the beginning or end are percentage columns\n",
    "    Need to add the exception for %\n",
    "    \n",
    "5. Challenges -\n",
    "    - False positives in model based NER\n",
    "    \n",
    "6. Future scope and experimentation\n",
    "    - Try 200d vectors\n",
    "    - Try 300d vectors\n",
    "    - Try 768d BERT embeddings (non context specific word embeddings)\n",
    "    - Try extracting phrase (in this case column names) embeddings as a whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "\n",
    "def model_based_ner(string: str) -> list:\n",
    "    word = nlp(column_name)\n",
    "    entity_list = []\n",
    "    for token_num, token in enumerate(word):\n",
    "#         print(f\"{token.text} -> {token.ent_iob_} -> {token.ent_type_}\")\n",
    "        if token.ent_type_:\n",
    "            entity_list.append(token.ent_type_)\n",
    "    \n",
    "    return entity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "\n",
    "@dataclass\n",
    "class CorpusSearch:\n",
    "    column_name: str\n",
    "    word_corpus: List[dict]\n",
    "    word_matrix: np.ndarray\n",
    "    confidence_required: float\n",
    "    embedding_model: KeyedVectors\n",
    "    entity_from_corpus: list = field(default_factory=list)\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        word_split = self.column_name.split(\" \")\n",
    "        for word_num, word in enumerate(word_split):\n",
    "            self.word_id = word_num\n",
    "            self.root_word = word\n",
    "            try:\n",
    "                self.root_word_embedding = self.embedding_model.get_vector(self.root_word)\n",
    "            except KeyError:\n",
    "                self.root_word_embedding = np.zeros((100,))\n",
    "            self.entity_from_corpus.extend(self.find_in_corpus())\n",
    "            \n",
    "        # Check if last word has empty entity\n",
    "        last_sub_word_entity = list(filter(\n",
    "            lambda word_is: (word_is.get(\"actual_word\") == word_split[-1]) and (not word_is.get(\"entity\")), \n",
    "            self.entity_from_corpus))\n",
    "#         print('Last word entity -> ', last_sub_word_entity)\n",
    "        # If non-empty, then check this word in corpus\n",
    "        if last_sub_word_entity:\n",
    "            theta_list = []\n",
    "            word_id = last_sub_word_entity[-1].get(\"word_id\")\n",
    "            self.last_word_embedding = self.root_word_embedding.squeeze()[:100]\n",
    "            closest_index = self.get_closest_word()\n",
    "\n",
    "            if closest_index or closest_index == 0:\n",
    "                closest_entity = self.word_corpus[closest_index] if (closest_index or closest_index == 0) else \"\"\n",
    "                print(closest_entity)\n",
    "                self.entity_from_corpus[-1].update({\"actual_word\": word_split[-1],\n",
    "                                               \"entity\": [closest_entity[\"entity\"]],\n",
    "                                               \"closest_root\": closest_entity[\"name\"],\n",
    "                                               \"word_id\": word_id})\n",
    "            else:\n",
    "                pass\n",
    "#                 print(f\"--------\\nNo entities found!\\n--------\")\n",
    "\n",
    "#             print('Updated entity object -> ', self.entity_from_corpus)\n",
    "            \n",
    "    def find_in_corpus(self) -> list:\n",
    "        entity_list = []\n",
    "        filtered_list = list(filter(lambda word_meta: word_meta[\"name\"] == self.root_word, \n",
    "                                    self.word_corpus))\n",
    "        meta_obj = {\n",
    "            \"actual_word\": self.root_word,\n",
    "            \"entity\": [],\n",
    "            \"closest_root:\": self.root_word,\n",
    "            \"word_id\": self.word_id\n",
    "        }\n",
    "        if filtered_list:\n",
    "            for each_object in filtered_list:\n",
    "                meta_obj[\"entity\"].append(each_object.get(\"entity\"))\n",
    "            entity_list.append(meta_obj)\n",
    "        else:\n",
    "            entity_list = [{\"actual_word\": self.root_word,\n",
    "                            \"entity\": [],\n",
    "                            \"closest_root\": \"\",\n",
    "                            \"word_id\": self.word_id}]\n",
    "\n",
    "        return entity_list\n",
    "    \n",
    "    @staticmethod\n",
    "    def embedding_product(a, b):\n",
    "        cos_theta = a.dot(b.T)/(np.sqrt(np.sum(np.square(a)))*(np.sqrt(np.sum(np.square(b)))))\n",
    "        _angle = math.acos(cos_theta)\n",
    "\n",
    "        return cos_theta, _angle\n",
    "\n",
    "\n",
    "    def get_closest_word(self):\n",
    "        theta_list = []\n",
    "        angle_list = []\n",
    "        for column_vec in range(self.word_matrix.T.shape[1]):\n",
    "            doc_product, angle_between_vectors = self.embedding_product(self.last_word_embedding, \n",
    "                                                                        self.word_matrix.T[:, column_vec])\n",
    "            theta_list.append(doc_product)\n",
    "            angle_list.append(angle_between_vectors)  # If needed for validation\n",
    "\n",
    "        # Cut-off: filter theta list based on the confidence required\n",
    "        filtered_theta_index_list = [\n",
    "            theta_list.index(score) for score in theta_list if np.abs(score)>=self.confidence_required]\n",
    "\n",
    "        # Find index of the top score\n",
    "        closest_vector = None\n",
    "        if filtered_theta_index_list:\n",
    "            closest_vector = theta_list.index(max([theta_list[filter_index] for filter_index in filtered_theta_index_list]))\n",
    "        print(\"closest_vect\",closest_vector)\n",
    "    #     print(theta_list)\n",
    "        print(theta_list.index(max(theta_list)))\n",
    "    #     print(theta_list[theta_list.index(max(theta_list))])\n",
    "    #     print(angle_list.index(min(angle_list)))\n",
    "\n",
    "        return closest_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "\n",
    "def construct_corpus_matrix(word_corpus: List[dict], embedding_model):\n",
    "    word_array = np.empty([len(word_corpus), embedding_model[embedding_model.index_to_key[0]].shape[0]])\n",
    "    for row_number, root in enumerate(word_corpus):\n",
    "        try:\n",
    "            word_array[row_number, :] = embedding_model.get_vector(root.get(\"name\"))\n",
    "        except KeyError:\n",
    "            word_array[row_number, :] = np.zeros([100,])\n",
    "        \n",
    "    return word_array\n",
    "\n",
    "# corpus_array = construct_corpus_matrix(intermediate_corpus, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model NER ->  []\n",
      "closest_vect 0\n",
      "0\n",
      "{'id': 0, 'name': 'date', 'entity': 'Date'}\n",
      "Search in Corpus ->  [{'actual_word': 'disbursed', 'entity': ['Sales'], 'closest_root:': 'disbursed', 'word_id': 0}, {'actual_word': 'count', 'entity': ['Date'], 'closest_root': 'date', 'word_id': 1}]\n",
      "['Sales', 'Date']\n"
     ]
    }
   ],
   "source": [
    "# Step 0\n",
    "corpus_array = construct_corpus_matrix(intermediate_corpus, model)\n",
    "# print(corpus_array.shape)\n",
    "\n",
    "entity_from_old_method = []\n",
    "entity_from_model_list = []\n",
    "entity_from_search = []\n",
    "for column_name in entity_data[\"column_names\"].values:\n",
    "    old_entity_list = [\n",
    "        word_obj[\"entity\"] for word_obj in list(filter(lambda corpus: corpus.get(\"name\") == column_name, old_entity_corpus))\n",
    "    ]\n",
    "#     print(\"old entities\", old_entity_list)\n",
    "    old_entity_list = list(set(old_entity_list))\n",
    "    if not old_entity_list:\n",
    "        old_entity_list = [\"\"]\n",
    "    entity_from_old_method.append(old_entity_list)\n",
    "    \n",
    "    column_name =  re.sub(r'[@_!#$%^&*()<>?[\\]./\\\\+|}{~:-]', ' ', column_name)  # Removal of special characters\n",
    "    column_name = re.sub(r\"[ ]{2,}\", \" \", column_name)  # Remove additional spaces\n",
    "    # Step 1 \n",
    "    entity_from_model = model_based_ner(column_name)\n",
    "    print('Model NER -> ', entity_from_model)\n",
    "    if entity_from_model:\n",
    "        entity_from_model = list(set(entity_from_model))\n",
    "    else:\n",
    "        entity_from_model = [\"\"]\n",
    "    entity_from_model_list.append(entity_from_model)\n",
    "\n",
    "    # Step 2: Find in corpus\n",
    "    corpus_search_obj = CorpusSearch(\n",
    "        column_name=column_name,\n",
    "        word_corpus=intermediate_corpus,\n",
    "        word_matrix=corpus_array,\n",
    "        confidence_required=0.,\n",
    "        embedding_model=model,\n",
    "    )\n",
    "\n",
    "    print('Search in Corpus -> ', corpus_search_obj.entity_from_corpus)\n",
    "\n",
    "    # Check if no entities found\n",
    "    non_identified_entity = list(filter(lambda word_is: not word_is.get(\"entity\"), \n",
    "                                        corpus_search_obj.entity_from_corpus))\n",
    "    if non_identified_entity and (len(non_identified_entity) == len(column_name.split(\" \"))):\n",
    "        print(f\"--------\\nNo entities found!\\n--------\")\n",
    "\n",
    "    extract_entities = [word_obj[\"entity\"] for word_obj in corpus_search_obj.entity_from_corpus]\n",
    "    extract_entities = [word for word_list in extract_entities for word in word_list]\n",
    "    print(extract_entities)\n",
    "    \n",
    "    entity_from_search.append(\", \".join(extract_entities))\n",
    "    for i, word in enumerate(entity_from_search):\n",
    "        if word == \"\":\n",
    "            entity_from_search.pop(i)\n",
    "            entity_from_search.insert(i, \"Default\")\n",
    "\n",
    "\n",
    "entity_from_old_method = [word for word_list in entity_from_old_method for word in word_list]\n",
    "entity_from_model_list = [word for word_list in entity_from_model_list for word in word_list]\n",
    "\n",
    "entity_data[\"old_recognition_method\"] = entity_from_old_method\n",
    "entity_data[\"model_based_recognition\"] = entity_from_model_list\n",
    "entity_data[\"new_algorithm_based_recognition\"] = entity_from_search\n",
    "\n",
    "# if not entity_from_model:\n",
    "#     word_split = column_name.split(\" \")\n",
    "#     # Step 2.a: Find sub_words in corpus\n",
    "#     entity_from_corpus = []\n",
    "#     for word_num, word in enumerate(word_split):\n",
    "#         entity_from_corpus.extend(find_in_corpus(word, word_num, intermediate_corpus))  # Can be replaced using a mat mul\n",
    "#     print('Simple search in Corpus -> ', entity_from_corpus)\n",
    "    \n",
    "#     # Step 2.b: Find if the last sub_word has returned an entity\n",
    "#     # Note: There can be 4 possibilities:\n",
    "#         # 1. All sub words can return entity\n",
    "#         # 2. Any sub_word other than the trailing sub_word returns an entity\n",
    "#         # 3. Any sub_word including the trailing sub_word returns an entity\n",
    "#         # 4. None of them return an entity\n",
    "    \n",
    "#     # 2.b.1: Check if all words contain entity\n",
    "#     non_identified_entity = list(filter(lambda word_is: not word_is.get(\"entity\"), entity_from_corpus))\n",
    "#     if non_identified_entity and (len(non_identified_entity) == len(word_split)):\n",
    "#         print(f\"--------\\nNo entities found!\\n--------\")\n",
    "        \n",
    "#     # 2.b.2:\n",
    "    \n",
    "    \n",
    "#     # 2.b.2: Check if last word has empty entity\n",
    "#     last_sub_word_entity = list(filter(\n",
    "#         lambda word_is: (word_is.get(\"actual_word\") == word_split[-1]) and (not word_is.get(\"entity\")), \n",
    "#         entity_from_corpus))\n",
    "#     print('Last word entity -> ', last_sub_word_entity)\n",
    "    \n",
    "#     # If non-empty, then check this word in corpus\n",
    "#     if last_sub_word_entity:\n",
    "#         theta_list = []\n",
    "#         word_id = last_sub_word_entity[-1].get(\"word_id\")\n",
    "#         last_word_embedding = model.get_vector(word_split[-1])\n",
    "#         closest_index = get_closest_word(last_word_embedding.squeeze()[:100], corpus_array, 0.3)\n",
    "        \n",
    "#         if closest_index:\n",
    "#             closest_entity = intermediate_corpus[closest_index] if (closest_index or closest_index == 0) else \"\"\n",
    "#             entity_from_corpus[-1].update({\"actual_word\": word_split[-1],\n",
    "#                                            \"entity\": [closest_entity[\"entity\"]],\n",
    "#                                            \"closest_root\": closest_entity[\"name\"],\n",
    "#                                            \"word_id\": word_id})\n",
    "#         else:\n",
    "#             print(f\"--------\\nNo entities found!\\n--------\")\n",
    "        \n",
    "#         print('Updated entity object -> ', entity_from_corpus)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# # corpus_array = construct_corpus_matrix(intermediate_corpus, model)\n",
    "# # corpus_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_data.to_excel(\"validation_output_final.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'actual_word': 'kk', 'entity': [], 'closest_root': '', 'word_id': 0},\n",
       " {'actual_word': 'kk', 'entity': [], 'closest_root': '', 'word_id': 1}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_search_obj.entity_from_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 26, 'name': 'company', 'entity': 'Organisation'}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intermediate_corpus[26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_vector(\"name\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.83234936 0.5874632370639816\n"
     ]
    }
   ],
   "source": [
    "sneaker = model.get_vector(\"man\")\n",
    "shoes = model.get_vector(\"woman\")\n",
    "\n",
    "dot_prod, angle = vector_cosine(sneaker, shoes)\n",
    "print(dot_prod, angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3502c2d8a44f1d370a9f7e349ee408832b67c21c31bd8924b7b4cde15de44893"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('nlp_env': venv)",
   "language": "python",
   "name": "python391jvsc74a57bd03d2a12883df202f51a08e1f757b345e71b08bf7c8dc91b1538c0cba0bebac7b3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
