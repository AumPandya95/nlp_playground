{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import re\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n",
    "\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "# Transformer libraries\n",
    "from sentence_transformers import SentenceTransformer # For estimating the distance between (sub)sequences\n",
    "from sentence_transformers import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate SBERT\n",
    "sentence_model = SentenceTransformer('all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spacy language model\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "# Load the GloVe embeddings\n",
    "# glove_file = datapath('/home/aum/Desktop/projects/nlp/models/glove.6B.100d.txt')\n",
    "glove_file = datapath('/home/aumaron/Desktop/nlp/nlp_playground/models/glove.6B/glove.6B.100d.txt')\n",
    "word2vec_glove_file = get_tmpfile(\"glove.6B.100d.word2vec.txt\")\n",
    "glove2word2vec(glove_file, word2vec_glove_file)\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(word2vec_glove_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dimensional space of the embeddings\n",
    "\n",
    "model[model.index_to_key[0]].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_data = pd.read_excel(\"../data/entity_data.xlsx\")\n",
    "root_word_corpus = pd.read_excel(\"../data/root_word_corpus.xlsx\")\n",
    "column_names = entity_data[\"column_names\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_corpus = root_word_corpus[['id', 'name', 'entity']].to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kk kk\n"
     ]
    }
   ],
   "source": [
    "column_name = 'kk+kk'\n",
    "column_name =  re.sub(r'[@_!#$%^&*()<>?[\\]./\\\\+|}{~:-]', ' ', column_name)  # Removal of special characters\n",
    "column_name = re.sub(r\"[ ]{2,}\", \" \", column_name)  # Remove additional spaces\n",
    "print(column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "query_embedding = sentence_model.encode([column_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Find NER using Spacy\n",
    "2. Find individual tokens in intermediate corpus\n",
    "3. If not found in step 2, find semantically similar words in R^d 100 dimensional space\n",
    "4. Future scope:\n",
    "    % is removed as a character\n",
    "    Certain columns containing '%' in the beginning or end are percentage columns\n",
    "    Need to add the exception for %\n",
    "    \n",
    "5. Challenges -\n",
    "    - False positives in model based NER\n",
    "    \n",
    "6. Future scope and experimentation\n",
    "    - Try 200d vectors\n",
    "    - Try 300d vectors\n",
    "    - Try 768d BERT embeddings (non context specific word embeddings)\n",
    "    - Try extracting phrase (in this case column names) embeddings as a whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "\n",
    "def model_based_ner(string: str) -> list:\n",
    "    word = nlp(column_name)\n",
    "    entity_list = []\n",
    "    for token_num, token in enumerate(word):\n",
    "        print(f\"{token.text} -> {token.ent_iob_} -> {token.ent_type_}\")\n",
    "        if token.ent_type_:\n",
    "            entity_list.append(token.ent_type_)\n",
    "    \n",
    "    return entity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "\n",
    "@dataclass\n",
    "class CorpusSearch:\n",
    "    word_id: str\n",
    "    root_word: str\n",
    "    root_word_embedding: np.ndarray\n",
    "    word_corpus: List[dict]\n",
    "    word_matrix: np.ndarray\n",
    "    confidence_required: float\n",
    "    \n",
    "\n",
    "    def find_in_corpus(root_word: str, word_id: int, word_corpus: List[dict]) -> list:\n",
    "        entity_list = []\n",
    "        filtered_list = list(filter(lambda word_meta: word_meta[\"name\"] == root_word, word_corpus))\n",
    "        meta_obj = {\n",
    "            \"actual_word\": root_word,\n",
    "            \"entity\": [],\n",
    "            \"closest_root:\": root_word,\n",
    "            \"word_id\": word_id\n",
    "        }\n",
    "        if filtered_list:\n",
    "            for each_object in filtered_list:\n",
    "                meta_obj[\"entity\"].append(each_object.get(\"entity\"))\n",
    "            entity_list.append(meta_obj)\n",
    "        else:\n",
    "            entity_list = [{\"actual_word\": root_word,\n",
    "                            \"entity\": [],\n",
    "                            \"closest_root\": \"\",\n",
    "                            \"word_id\": word_id}]\n",
    "\n",
    "        return entity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "\n",
    "def construct_corpus_matrix(word_corpus: List[dict], embedding_model):\n",
    "    word_array = np.empty([len(word_corpus), embedding_model[embedding_model.index_to_key[0]].shape[0]])\n",
    "    for row_number, root in enumerate(word_corpus):\n",
    "        try:\n",
    "            word_array[row_number, :] = embedding_model.get_vector(root.get(\"name\"))\n",
    "        except KeyError:\n",
    "            word_array[row_number, :] = np.zeros([100,])\n",
    "        \n",
    "    return word_array\n",
    "\n",
    "# corpus_array = construct_corpus_matrix(intermediate_corpus, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "\n",
    "def embedding_product(a, b):\n",
    "    cos_theta = a.dot(b.T)/(np.sqrt(np.sum(np.square(a)))*(np.sqrt(np.sum(np.square(b)))))\n",
    "    _angle = math.acos(cos_theta)\n",
    "\n",
    "    return cos_theta, _angle\n",
    "\n",
    "\n",
    "def get_closest_word(root_word_embedding: np.ndarray, \n",
    "                     word_matrix: np.ndarray,\n",
    "                     confidence_required: Optional[float] = 0.5):\n",
    "    theta_list = []\n",
    "    angle_list = []\n",
    "    for column_vec in range(word_matrix.T.shape[1]):\n",
    "        doc_product, angle_between_vectors = embedding_product(root_word_embedding, word_matrix.T[:, column_vec])\n",
    "        theta_list.append(doc_product)\n",
    "        angle_list.append(angle_between_vectors)  # If needed for validation\n",
    "    \n",
    "    # Cut-off: filter theta list based on the confidence required\n",
    "    filtered_theta_index_list = [theta_list.index(score) for score in theta_list if np.abs(score)>=confidence_required]\n",
    "    \n",
    "    # Find index of the top score\n",
    "    closest_vector = None\n",
    "    if filtered_theta_index_list:\n",
    "        closest_vector = theta_list.index(max([theta_list[filter_index] for filter_index in filtered_theta_index_list]))\n",
    "        \n",
    "#     print(theta_list)\n",
    "#     print(theta_list.index(max(theta_list)))\n",
    "#     print(theta_list[theta_list.index(max(theta_list))])\n",
    "#     print(angle_list.index(min(angle_list)))\n",
    "    \n",
    "    return closest_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kk -> O -> \n",
      "kk -> O -> \n",
      "Model NER ->  []\n",
      "Simple search in Corpus ->  [{'actual_word': 'kk', 'entity': [], 'closest_root': '', 'word_id': 0}, {'actual_word': 'kk', 'entity': [], 'closest_root': '', 'word_id': 1}]\n",
      "--------\n",
      "No entities found!\n",
      "--------\n",
      "Last word entity ->  [{'actual_word': 'kk', 'entity': [], 'closest_root': '', 'word_id': 0}, {'actual_word': 'kk', 'entity': [], 'closest_root': '', 'word_id': 1}]\n",
      "--------\n",
      "No entities found!\n",
      "--------\n",
      "Updated entity object ->  [{'actual_word': 'kk', 'entity': [], 'closest_root': '', 'word_id': 0}, {'actual_word': 'kk', 'entity': [], 'closest_root': '', 'word_id': 1}]\n"
     ]
    }
   ],
   "source": [
    "# Step 0\n",
    "corpus_array = construct_corpus_matrix(intermediate_corpus, model)\n",
    "# print(corpus_array.shape)\n",
    "\n",
    "# Step 1 \n",
    "entity_from_model = model_based_ner(column_name)\n",
    "print('Model NER -> ', entity_from_model)\n",
    "\n",
    "# Step 2: Find in corpus\n",
    "if not entity_from_model:\n",
    "    word_split = column_name.split(\" \")\n",
    "    # Step 2.a: Find sub_words in corpus\n",
    "    entity_from_corpus = []\n",
    "    for word_num, word in enumerate(word_split):\n",
    "        entity_from_corpus.extend(find_in_corpus(word, word_num, intermediate_corpus))  # Can be replaced using a mat mul\n",
    "    print('Simple search in Corpus -> ', entity_from_corpus)\n",
    "    \n",
    "    # Step 2.b: Find if the last sub_word has returned an entity\n",
    "    # Note: There can be 4 possibilities:\n",
    "        # 1. All sub words can return entity\n",
    "        # 2. Any sub_word other than the trailing sub_word returns an entity\n",
    "        # 3. Any sub_word including the trailing sub_word returns an entity\n",
    "        # 4. None of them return an entity\n",
    "    \n",
    "    # 2.b.1: Check if all words contain entity\n",
    "    non_identified_entity = list(filter(lambda word_is: not word_is.get(\"entity\"), entity_from_corpus))\n",
    "    if non_identified_entity and (len(non_identified_entity) == len(word_split)):\n",
    "        print(f\"--------\\nNo entities found!\\n--------\")\n",
    "        \n",
    "    # 2.b.2:\n",
    "    \n",
    "    \n",
    "    # 2.b.2: Check if last word has empty entity\n",
    "    last_sub_word_entity = list(filter(\n",
    "        lambda word_is: (word_is.get(\"actual_word\") == word_split[-1]) and (not word_is.get(\"entity\")), \n",
    "        entity_from_corpus))\n",
    "    print('Last word entity -> ', last_sub_word_entity)\n",
    "    \n",
    "    # If non-empty, then check this word in corpus\n",
    "    if last_sub_word_entity:\n",
    "        theta_list = []\n",
    "        word_id = last_sub_word_entity[-1].get(\"word_id\")\n",
    "        last_word_embedding = model.get_vector(word_split[-1])\n",
    "        closest_index = get_closest_word(last_word_embedding.squeeze()[:100], corpus_array, 0.3)\n",
    "        \n",
    "        if closest_index:\n",
    "            closest_entity = intermediate_corpus[closest_index] if (closest_index or closest_index == 0) else \"\"\n",
    "            entity_from_corpus[-1].update({\"actual_word\": word_split[-1],\n",
    "                                           \"entity\": [closest_entity[\"entity\"]],\n",
    "                                           \"closest_root\": closest_entity[\"name\"],\n",
    "                                           \"word_id\": word_id})\n",
    "        else:\n",
    "            print(f\"--------\\nNo entities found!\\n--------\")\n",
    "        \n",
    "        print('Updated entity object -> ', entity_from_corpus)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# corpus_array = construct_corpus_matrix(intermediate_corpus, model)\n",
    "# corpus_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 26, 'name': 'company', 'entity': 'Organisation'}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intermediate_corpus[26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.83234936 0.5874632370639816\n"
     ]
    }
   ],
   "source": [
    "sneaker = model.get_vector(\"man\")\n",
    "shoes = model.get_vector(\"woman\")\n",
    "\n",
    "dot_prod, angle = vector_cosine(sneaker, shoes)\n",
    "print(dot_prod, angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3502c2d8a44f1d370a9f7e349ee408832b67c21c31bd8924b7b4cde15de44893"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('nlp_env': venv)",
   "language": "python",
   "name": "python391jvsc74a57bd03d2a12883df202f51a08e1f757b345e71b08bf7c8dc91b1538c0cba0bebac7b3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
