{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import re\n",
    "import math\n",
    "from typing import List\n",
    "\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_file = datapath('/home/aum/Desktop/projects/nlp/models/glove.6B.100d.txt')\n",
    "# glove_file = datapath('/home/aumaron/Desktop/nlp/nlp_playground/models/glove.6B/glove.6B.100d.txt')\n",
    "word2vec_glove_file = get_tmpfile(\"glove.6B.100d.word2vec.txt\")\n",
    "glove2word2vec(glove_file, word2vec_glove_file)\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(word2vec_glove_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[model.index_to_key[0]].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_data = pd.read_excel(\"../data/entity_data.xlsx\")\n",
    "root_word_corpus = pd.read_excel(\"../data/root_word_corpus.xlsx\")\n",
    "column_names = entity_data[\"column_names\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_corpus = root_word_corpus[['id', 'name', 'entity']].to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taluka name\n"
     ]
    }
   ],
   "source": [
    "column_name = 'taluka_name'\n",
    "column_name =  re.sub(r'[@_!#$%^&*()<>?[\\]./\\\\|}{~:-]', ' ', column_name)  # Removal of special characters\n",
    "column_name = re.sub(r\"[ ]{2,}\", \" \", column_name)  # Remove additional spaces\n",
    "print(column_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Find NER using Spacy\n",
    "2. Find individual tokens in intermediate corpus\n",
    "3. If not found in step 2, find semantically similar words in R^d 100 dimensional space\n",
    "4. Future scope:\n",
    "    % is removed as a character\n",
    "    Certain columns containing '%' in the beginning or end are percentage columns\n",
    "    Need to add the exception for %\n",
    "    \n",
    "5 Challenges -\n",
    "    - latitude-longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "\n",
    "def model_based_ner(string: str) -> list:\n",
    "    word = nlp(column_name)\n",
    "    entity_list = []\n",
    "    for token in word:\n",
    "#         print(f\"{token.ent_iob_} -> {token.ent_type_}\")\n",
    "        if token.ent_type_:\n",
    "            entity_list.append(token.ent_type_)\n",
    "    \n",
    "    return entity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "\n",
    "def find_in_corpus(root_word: str, word_corpus: List[dict]) -> list:\n",
    "    entity_list = []\n",
    "    filtered_list = list(filter(lambda word_meta: word_meta[\"name\"] == root_word, word_corpus))\n",
    "    if filtered_list:\n",
    "        for each_object in filtered_list:\n",
    "            entity_list.append({each_object.get(\"name\"): each_object.get(\"entity\")})\n",
    "    else:\n",
    "        entity_list = [{root_word: \"\"}]\n",
    "    \n",
    "    return entity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "\n",
    "def construct_corpus_matrix(word_corpus: List[dict], embedding_model):\n",
    "    word_array = np.empty([len(word_corpus), embedding_model[embedding_model.index_to_key[0]].shape[0]])\n",
    "    for row_number, root in enumerate(word_corpus):\n",
    "        try:\n",
    "            word_array[row_number, :] = embedding_model.get_vector(root.get(\"name\"))\n",
    "        except KeyError:\n",
    "            word_array[row_number, :] = np.zeros([100,])\n",
    "        \n",
    "    return word_array\n",
    "\n",
    "# corpus_array = construct_corpus_matrix(intermediate_corpus, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "\n",
    "def vector_cosine(a, b):\n",
    "    cos_theta = a.dot(b.T)/(np.sqrt(np.sum(np.square(a)))*(np.sqrt(np.sum(np.square(b)))))\n",
    "    _angle = math.acos(cos_theta)\n",
    "\n",
    "    return cos_theta, _angle\n",
    "\n",
    "\n",
    "def get_closest_word(root_word_embedding: np.ndarray, \n",
    "                     matrix: np.ndarray):\n",
    "    theta_list = []\n",
    "    angle_list = []\n",
    "    for column_vec in range(matrix.T.shape[1]):\n",
    "        doc_product, angle_between_vectors = vector_cosine(root_word_embedding, matrix.T[:, column_vec])\n",
    "        theta_list.append(doc_product)\n",
    "        angle_list.append(angle_between_vectors)\n",
    "    # TODO: Can add cut-offs\n",
    "    print(theta_list)\n",
    "    print(theta_list.index(max(theta_list)))\n",
    "    print(theta_list[theta_list.index(max(theta_list))])\n",
    "    print(angle_list.index(min(angle_list)))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(66, 100)\n",
      "['PERSON']\n"
     ]
    }
   ],
   "source": [
    "# Step 0\n",
    "corpus_array = construct_corpus_matrix(intermediate_corpus, model)\n",
    "print(corpus_array.shape)\n",
    "\n",
    "# Step 1 \n",
    "entity_from_model = model_based_ner(column_name)\n",
    "print(entity_from_model)\n",
    "\n",
    "# Step 2: Find in corpus\n",
    "if not entity_from_model:\n",
    "    word_split = column_name.split(\" \")\n",
    "    # Step 2.a: Find sub_words in corpus\n",
    "    entity_from_corpus = []\n",
    "    for word in word_split:\n",
    "        entity_from_corpus.extend(find_in_corpus(word, intermediate_corpus))  # Can be replaced using a mat mul\n",
    "    print(entity_from_corpus)\n",
    "    \n",
    "    # Step 2.b: Find if the last sub_word has returned an entity\n",
    "    # Note: There can be 4 possibilities:\n",
    "        # 1. All sub words can return entity\n",
    "        # 2. Any sub_word other than the trailing sub_word returns an entity\n",
    "        # 3. Any sub_word including the trailing sub_word returns an entity\n",
    "        # 4. None of them return an entity\n",
    "    \n",
    "    # Check if last word has empty entity\n",
    "    last_sub_word_entity = list(filter(lambda word_is: word_is.get(word_split[-1]) == \"\", entity_from_corpus))\n",
    "    print(last_sub_word_entity)\n",
    "    \n",
    "    # If non-empty, then check this word in corpus\n",
    "    if last_sub_word_entity:\n",
    "        theta_list = []\n",
    "        last_word_embedding = model.get_vector(word_split[-1])\n",
    "        get_closest_word(last_word_embedding, corpus_array)\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# corpus_array = construct_corpus_matrix(intermediate_corpus, model)\n",
    "# corpus_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 60, 'name': 'profit', 'entity': 'Revenue'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intermediate_corpus[60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('nlp_env': venv)",
   "language": "python",
   "name": "python391jvsc74a57bd03502c2d8a44f1d370a9f7e349ee408832b67c21c31bd8924b7b4cde15de44893"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
